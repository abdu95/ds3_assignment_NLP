---
title: "DS3 - Unstructured text analysis"
subtitle: "Author: Abduvosid Malikov"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}

library(dplyr)
library(stringr)
library(topicmodels)
library(tidytext)
library(textdata)

library(ggplot2)
library(lubridate)
library(tidyr)
# install.packages("ggpubr")
library(ggpubr)
library(knitr)
library(kableExtra)
library(gridExtra)
library(cowplot)
library(forcats)

```

# Let's make customer support great again

### Introduction

NLP itself is a big sphere. It was difficult for me to find a topic and dataset to do this task. Then I was inspired by these lines in Tidytext book, chapter 6:
 
*... vandal has torn the books into individual chapters, and left them in one large pile. How can we restore these disorganized chapters to their original books? *


### Data Gathering 

Therefore, I decided to do my home assignment related to topic modeling. As a data source, I chose Upwork - an American freelancing platform where freelancers and clients connect in order to conduct business. 

In Upwork Community forum, people publish their questions in the form of posts. There are separate pages for different members: freelancers, clients, agencies, first-timers. 



```{r member-logo, echo = FALSE, fig.align = 'center', out.width = "55%", fig.cap = "Upwork Communit forum: https://community.upwork.com/"}
knitr::include_graphics(here::here("members.png"))
```


Each post has title (1) & content (2) (see image below). I scraped these Upwork forum posts and and planned to label each post with topics using *topic modeling*. I also planned to make sentiment analysis using this data.  


```{r posts-figure, echo = FALSE, fig.align = 'center', out.width = "55%", fig.cap = "Upwork posts: https://community.upwork.com/"}
knitr::include_graphics(here::here("index.png"))
```



I scraped [clients](https://community.upwork.com/t5/Clients/bd-p/clients) and [freelancers](https://community.upwork.com/t5/Freelancers/bd-p/freelancers) web-pages and saved them as CSV files. For web scraping, I used Python, Beautiful Soup and other libraries in Jupyter notebook. Even though I admit that rvest, httr, requests are good R alternatives, I prefer using Python for web-scraping. For NLP part of this task, I used R, it's tidytext and other corresponding libraries. 


From clients page, 369 web pages were scraped and  from freelancers page 864 web pages were scraped. Clients data contains 5539 observations and freelancers data contains 12965 observations. This inconsistency comes from the fact that: 

- there are more posts in freelancers page
- clients page was more difficult to scrape, script took 2 days to complete, stuck several times. 

```{r, message=FALSE, warning=FALSE}
df_clients <- read.csv('D:/CEU/singer semester/unstructured-text-analysis/Final-Project-DS3/clients_data.csv', header = FALSE, encoding = "UTF-8")

df_freelancers <- read.csv('D:/CEU/singer semester/unstructured-text-analysis/Final-Project-DS3/freelancers_data.csv', header = FALSE, encoding = "UTF-8")
```



### Data cleaning


```{r,  message=FALSE, warning=FALSE}
names(df_clients)[1] <- "post_title"
names(df_clients)[2] <- "post_date"
names(df_clients)[3] <- "post_text"


names(df_freelancers)[1] <- "post_title"
names(df_freelancers)[2] <- "post_date"
names(df_freelancers)[3] <- "post_text"

```


Scraped data for clients and freelancers was saved in separate CSV files. CSV file contained 3 columns: post_title, post_date, and	post_text

```{r, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(head(df_clients, 3), caption = "Dataframe with 3 columns", 
             table.attr = "style='width:90%;'") %>% 
  kableExtra::kable_styling() 

```

Since post could consist of several paragraphs, new lines existed in the text. They were removed using string function and Regular Expression.

Also, post_date was given appropriate date data type using lubridate. 

Now, our data is in text (untidy) format, Later when we turn it into tidytext format, we want some identifier to uniquely identify each post. Therefore, we add post_number column that has value of each row number. 

```{r, message=FALSE, warning=FALSE}
# remove line breaks
for(i in 1:nrow(df_clients)){
  df_clients[i, ] <- str_replace_all(df_clients[i, ], "[\n\t]" , "")
}

for(i in 1:nrow(df_freelancers)){
  df_freelancers[i, ] <- str_replace_all(df_freelancers[i, ], "[\n\t]" , "")
}

# from string to date
df_clients$post_date <- mdy_hms(df_clients$post_date)
df_freelancers$post_date <- mdy_hms(df_freelancers$post_date)

# add post number
df_post_n <- df_clients %>%
  mutate(post_number = row_number())

df_f_post_n <- df_freelancers %>%
  mutate(post_number = row_number())

```


### Text analysis

In order to analyze the text, a raw string of text must be tokenized. There are also other adjustments that might need to be made to better analyze the data. The process of preparing the text for further analysis is the following: 

Dataframe --> **Word tokenization** --> Remove Numbers --> Remove Stop Words --> **Text Cleaning** --> **Text normalization** --> Dataframe

#### Tokenization


The first step is using the unnest_token function in the tidytext package to put each word in a separate row. Now clients data has dimensions 417902 rows and 4 columns (freelancers data - 990259 rows 4 columns). The unnest token function also performed text cleaning by converting all upper case letters to lower case and removing all special characters and punctuation.

```{r}

df_unnest <- df_post_n %>%
  unnest_tokens(word, post_text)

df_f_unnest <- df_f_post_n %>%
  unnest_tokens(word, post_text)

```

```{r clients_unnest, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(head(df_unnest, 3), caption = "Dataframe after tokenization", align = "lccrr", 
             table.attr = "style='width:90%;'") %>% 
  kableExtra::kable_styling() 
```


#### Removing Numbers

Numbers will not provide us any insight to sentiment analysis so we will remove them using the following code. Rows are reduced from 417902 to 412090 (974470 for freelancers).

```{r}

df_number <- df_unnest %>% 
  filter(!grepl('[0-9]', word)) 

df_f_number <- df_f_unnest %>% 
  filter(!grepl('[0-9]', word)) 
```


Before removing stop words from our corpus, let’s look at the top 10 most frequently used words in all posts: 


```{r, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(head(df_unnest %>%
  count(word, sort = T), 10), caption = "Top 10 most frequently used words", align = "lccrr",
  table.attr = "style='width:30%;'") %>% 
  kableExtra::kable_styling() 

```


The top 10 list is comprised of these words: i, to, the, a, and. These are the exact words that the stop word removal process is trying to eliminate - words that are frequently used but do not contribute to our analysis. We also see that there are *14709 unique words* used throughout the corpus.


```{r, warning=FALSE, message=FALSE}

df_stop <- df_number %>% 
  anti_join(stop_words) 

df_f_stop <- df_f_number %>% 
  anti_join(stop_words) 

```

#### Stop words

Removing the stop words reduces the list from 412090 to 137958 rows (for freelancers, from 974720 to 322275 rows). 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(head(df_stop %>%
  count(word, sort = T), 10), 
  caption = "Top 10 most frequently used words after removing stop words", align = "lccrr", 
  table.attr = "style='width:30%;'") %>% 
  kableExtra::kable_styling() 

```




The top 10 list of frequently used words changes drastically by removing the stop words but the lexical variety decreases slightly from *14709 words* to *13034 words*.

If we think about this. 1675 individual words accounted for 274132 of the total words used or almost 67%! (412090 total words after removing numbers, minus 137958 after stop words removed, equals 274132 / 412090 = 66.52%). 


#### Most common words

Now, let's visualize the most common words used in the posts and compare how it differs between clients and freelancers. 

```{r, warning=FALSE, message=FALSE}

common_words_clients <- df_stop %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  top_n(10) %>% 
  ggplot(aes(n, word)) +
  geom_col(fill = '#6fda44') +
  ggtitle("Posts by clients") +
  labs(y = NULL)


common_words_freelancers <- df_f_stop %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  top_n(10) %>% 
  ggplot(aes(n, word)) +
  geom_col(fill = '#6fda44') +
  ggtitle("Posts by freelancers") +
  labs(y = NULL)
```


```{r}
plot_grid(common_words_clients, common_words_freelancers, 
          align = "h", nrow = 1, rel_widths = c(1/5, 1/5))

```

We can see that both type of members mostly use almost same words: upwork, account, job, contract, project, time, payment. Top 10 list is finished with the word *pay* in case of clients and we don't see word *solution* in clients top list but exists in freelancers top list. Fun fact: second most used word by clients is *freelancer* and second most used word by freelancers is *client*. We may assume that it's because clients and freelancers poke each other in the forum. 


#### Sentiment Analysis 

Now when our text is in tidy format, we can make sentiment analysis on it. There are many ways for assessing the emotion in text. Further versions of this analysis can potentially use a sentiment lexicon specifically for freelancing and customer support. But we will use three general-purpose sentiment lexicons that tidytext package provides AFINN, bing, and nrc.

Using NRC lexicon, let's see what are the most common joy words in clients and freelancers post.  

```{r, message=FALSE, warning=FALSE}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

knitr::kables(
  list(
    knitr::kable(df_stop %>%
      inner_join(nrc_joy) %>%
      count(word, sort = TRUE) %>%
      top_n(10), caption = "Joy words for clients",
      format = "html", table.attr = "style='width:30%;'") %>%
      kableExtra::kable_styling(),

    knitr::kable(df_f_stop %>%
      inner_join(nrc_joy) %>%
      count(word, sort = TRUE) %>%
      top_n(10), caption = "Joy words for freelancers",
      format = "html", table.attr = "style='width:30%;'") %>%
      kableExtra::kable_styling()
  )
)

```



Since now we have data frame with both sentiment and word, why don't we analyze how much each word contributed to each sentiment: 


```{r, warning=FALSE, message=FALSE}

clients_bing_word_counts <- df_stop %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup() 

freelancers_bing_word_counts <- df_f_stop %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup() 

clients_word_counts_g <- clients_bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment by clients",
       y = NULL)

freelancers_word_counts_g <- freelancers_bing_word_counts %>% 
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment by freelancers",
     y = NULL)

plot_grid(clients_word_counts_g, freelancers_word_counts_g, 
          align = "h", nrow = 2, rel_heights = c(1/5, 1/5))

```

We don't see big difference in sentiments by freelancers and clients. For both issue, error, dispute, wrong, bad are the words most contributed to negative sentiment. Such words as refund, support, top, correct, free, happy contributed most to positive sentiment. 

These are the top 10 posts with most negative words. Basically, we can say these are the most unsatisfied clients who are potentially ready to leave Upwork platform. If Upwork cares about customer retention, managers can further investigate who had what problem and try to solve these issues faster than other issues using post number. 

```{r, warning=FALSE, message=FALSE}

client_sentiment <- df_stop %>%
  inner_join(get_sentiments("bing")) %>%
  count(post_title, post_number, post_date,  sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  arrange(sentiment) 

knitr::kable(client_sentiment %>% 
  slice(1:10), caption = "Top 10 posts with most negative words",
  format = "html", table.attr = "style='width:90%;'") %>%
  kableExtra::kable_styling()
```



##### Distribution

To see the big picture, we can see how words with negative and positive sentiment are distributed in the whole dataset. This plot shows the distribution with mean (red dashed line). We can see that the distribution is close to normal. But the sentiment is on average negative. This also can be explained by the fact that actually *the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon.*


```{r}

clients_bing_plot <- ggplot(client_sentiment, aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) + 
  ggtitle('Sentiment in clients text using bing') 
clients_bing_plot
```

But average sentiment in freelancers posts is less negative - maybe because we have more records for freelancers compared to clients. 

```{r, warning=FALSE, message=FALSE}

freelancers_bing_df <- df_f_stop %>%
  inner_join(get_sentiments("bing")) %>%
  count(post_title, post_number, post_date,  sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  arrange(sentiment)


freelancers_bing_plot <- ggplot(freelancers_bing_df, aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) +
    ggtitle('Sentiment in freelancers, using bing') 
freelancers_bing_plot
```


Different lexicons determining sentiment in different ways. Now let's see the sentiment in clients and freelancers posts assessed based on two other lexicons. 

```{r, warning=FALSE, message=FALSE}
clients_nrc_plot <- df_stop %>%
  inner_join(get_sentiments("nrc")) %>%
  count(post_title, post_number, post_date,  sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  arrange(sentiment) %>% 
  ggplot(aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) +
    ggtitle('Sentiment in clients text using NRC') 

freelancers_nrc_plot <- df_f_stop %>%
  inner_join(get_sentiments("nrc")) %>%
  count(post_title, post_number, post_date,  sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  arrange(sentiment) %>% 
  ggplot(aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) + 
    ggtitle('Sentiment in freelancers, using NRC') 



clients_afinn_plot <- df_stop %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(post_title) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN") %>% 
  ggplot(aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) +
    ggtitle('Sentiment in clients text using AFINN') 

freelancers_afinn_plot <- df_f_stop %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(post_title) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN") %>% 
  filter(sentiment < 60) %>% 
  ggplot(aes(x=sentiment)) + 
  geom_histogram(binwidth= 1, colour="black", fill="white") + 
  geom_vline(aes(xintercept=mean(sentiment, na.rm=T)),   
             color="red", linetype="dashed", size=1) +
    ggtitle('Sentiment in freelancers, using AFINN') 

```


NRC lexicon shows that average sentiment in both clients and freelancers text is less than 0 - negative. 


```{r}
plot_grid(clients_nrc_plot, freelancers_nrc_plot, 
          align = "h", ncol = 2, rel_widths = c(1/5, 1/5))
```

Average sentiment in clients and freelancers text is different based on AFINN. This lexicon shows that average sentiment in clients text is negative and average sentiment in freelancers text is positive. 


```{r}
plot_grid(clients_afinn_plot, freelancers_afinn_plot, 
          align = "h", ncol = 2, rel_widths = c(1/5, 1/5))
```




#### TF_IDF

TF_IDF helps us to see most used words and total words in each post. 


```{r, warning=FALSE, message=FALSE, results='hide'}
post_words_c <- df_stop %>% 
  count(post_title, word, sort = TRUE)

total_words_c <- post_words_c %>% 
  group_by(post_title) %>% 
  summarize(total = sum(n))

knitr::kable(head(total_words_c %>%
  count(total, sort = T), 10), 
  caption = "Most used words", align = "lccrr", 
  table.attr = "style='width:30%;'") %>% 
  kableExtra::kable_styling() 


```


```{r}

post_words_c <- left_join(post_words_c, total_words_c)

ggplot(post_words_c, aes(n/total)) +
  geom_histogram(show.legend = FALSE) + 
  scale_x_continuous(limits = c(0, 0.2))


```


The long tail to the right shown in this plot exhibits the term frequency distribution. It shows many words occur rarely and fewer words occur frequently.




```{r}
post_c_tf_idf <- post_words_c %>%
  bind_tf_idf(word, post_title, n)

knitr::kable(head( post_c_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf)), 10), 
  caption = "Words with high tf-idf ", align = "lccrr", 
  table.attr = "style='width:80%;'") %>% 
  kableExtra::kable_styling() 


```

Aim of tf-idf is to decrease the weight for common words and increase the weight for words that occur in fewer of the documents. Now we can look at terms with high tf-idf in the posts.

Table shows nouns, and business terms that are indeed important for participants.



####  LDA

Beside sentiment analysis, we can use our data for topic modeling as well. Topic modeling is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. 

We will use **topicmodels** package for topic modeling but it requires a DocumentTermMatrix. So we should case our one-token-per-row table into a DocumentTermMatrix with tidytext’s cast_dtm().


```{r}
#  document-word counts
clients_word_counts <- df_stop %>%
  count(post_title, word, sort = TRUE) %>%
  ungroup()

posts_dtm_c <- clients_word_counts %>%
  cast_dtm(post_title, word, n)

```

Then we use LDA() function from the topicmodels package, setting k = 4, to create a four-topic LDA model.

```{r}

posts_lda_c <- LDA(posts_dtm_c, k = 4, control = list(seed = 1234))

posts_topics_c <- tidy(posts_lda_c, matrix = "beta")

```

Then we can examine per-topic-per-word probabilities. This has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic.

After that we use dplyr’s top_n() to find the top 5 terms within each topic.


```{r}

top_terms_c <- posts_topics_c %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_c %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Right now, it's difficult to give a name to each topic. 1st topic mostly contains words contract, freelancer, money, refund, project. I would probably call this topic *contract* because it mainly is around this topic. 2nd topic mostly consists of words such as account, card, upwork, payment, method. I would call this topic *account* because payment and card is mainly about account. 3rd topic contains words upwork, freelancer, community, guidelines, edited. I call this topic *community* because it covers platform itself, guidelines and freelancer. 4th topic mostly consists of such words as job, upwork, freelancer, freelancers, post. I call this topic *freelancer* because the job is done by freelancers. 

Managers of Upwork can replicate and improve such analysis to divide the post forums into certain topics. Then they can assign corresponding expert in that topic to deal with the problem mentioned in that post. 



## Summary 

We learned how NLP can enhance business operations, particularly: 

1. To see most common words used by freelancers and clients

2. To see which words are contributing more to negative sentiment and solve such problems or to see which words are contributing more to positive sentiment and enhance such activities.

3. To see the distribution of sentiment between freelancers and clients


4. To find most unsatisfied clients with most negative sentiment and solve such critical issues first

5. To divide posts into topics with LDA topic modeling 


## References


1. 

Name: NRC Word-Emotion Association Lexicon 
URL: http://saifmohammad.com/WebPages/lexicons.html 

Saif M. Mohammad and Peter Turney. (2013),


2. Sentiment Analysis of 49 years of Warren Buffett’s Letters to Shareholders of Berkshire Hathaway. Source: https://bookdown.org/psonkin18/berkshire/ 

3. Text Mining with R. Source: https://www.tidytextmining.com/index.html


